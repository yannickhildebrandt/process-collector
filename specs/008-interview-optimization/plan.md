# Implementation Plan: Interview Assistant Optimization

**Branch**: `008-interview-optimization` | **Date**: 2026-02-24 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/008-interview-optimization/spec.md`

## Summary

Optimize the interview assistant in four areas: (1) batch summary extraction every 3 messages with incremental updates (only new messages + existing summary), (2) conversation windowing to send only the most recent 15 messages plus the summary as context, (3) cache project configuration and compiled system prompt per session, (4) polish the chat UI with markdown rendering in AI messages (progressive during streaming) and visual improvements (icons, better bubble styling). Adds a `lastSummarizedIndex` field to InterviewSession for tracking extraction progress. Updates the system prompt to instruct the AI to use markdown formatting.

## Technical Context

**Language/Version**: TypeScript 5.x, Node.js 22+
**Primary Dependencies**: Next.js 16.1.6, React 19, Vercel AI SDK v6, Prisma 6, react-markdown 10, remark-gfm 4, bpmn-js 18, next-intl 4
**Storage**: PostgreSQL via Prisma ORM (migration: add `lastSummarizedIndex` to InterviewSession)
**Testing**: Manual testing per quickstart.md checklist
**Target Platform**: Browser (desktop + mobile)
**Project Type**: Web application (Next.js App Router)
**Performance Goals**: 40%+ token reduction per interview, consistent latency for 20+ message interviews
**Constraints**: Client-side rendering for markdown, server-side caching for config
**Scale/Scope**: Modifications to chat route, summary extractor, prompt builder, message bubble component, interview page

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Notes |
|-----------|--------|-------|
| I. AI-Guided, Not AI-Replaced | PASS | No change to AI's role — still guided by employee input. Formatting instructions tell AI how to present, not what to invent. |
| II. Self-Service by Design | PASS | Rich formatting improves self-service UX — employees can scan AI guidance more easily. |
| III. Structured Output over Free Text | PASS | Summary extraction still produces structured ProcessSummary. Incremental updates preserve structure. |
| IV. Configuration as Data, Not Code | PASS | No configuration changes. Cache is transparent optimization. |
| V. LLM-Agnostic by Architecture | PASS | All changes go through the existing AI SDK abstraction. Formatting instructions are provider-neutral markdown. |

All gates pass. No violations.

## Project Structure

### Documentation (this feature)

```text
specs/008-interview-optimization/
├── plan.md              # This file
├── spec.md              # Feature specification
├── research.md          # Phase 0 output (7 decisions)
├── data-model.md        # Phase 1 output (1 schema change)
├── quickstart.md        # Phase 1 output (test checklist)
└── tasks.md             # Phase 2 output (generated by /speckit.tasks)
```

### Source Code (repository root)

```text
src/
├── app/api/projects/[projectId]/
│   ├── interviews/[interviewId]/chat/
│   │   └── route.ts                        # UPDATE: Batched extraction, windowing, config caching
│   └── configuration/
│       └── route.ts                        # UPDATE: Cache invalidation on config save
├── components/interview/
│   ├── chat-interface.tsx                  # UPDATE: Minor adjustments for streaming markdown
│   └── message-bubble.tsx                  # UPDATE: Markdown rendering, icons, visual polish
├── lib/
│   ├── interview/
│   │   ├── prompt-builder.ts              # UPDATE: Summary context injection, formatting instructions
│   │   ├── summary-extractor.ts           # UPDATE: Incremental extraction (new messages only)
│   │   └── config-cache.ts                # NEW: In-memory config cache with TTL
│   └── export/
│       └── diagram-export.ts              # (no changes)
└── i18n/messages/
    ├── en.json                             # (no changes expected)
    └── de.json                             # (no changes expected)

prisma/
└── schema.prisma                           # UPDATE: Add lastSummarizedIndex to InterviewSession
```

**Structure Decision**: All changes within the existing Next.js App Router structure. One new utility file (`config-cache.ts`) for the in-memory cache. One Prisma migration for the new field.

### Key Implementation Details

**Batched Summary Extraction**:
- New field `lastSummarizedIndex` (Int, default: -1) on InterviewSession
- In `onFinish` of chat route: check if `(nextOrderIndex + 1 - lastSummarizedIndex) >= 3`
- If true: extract summary, update `lastSummarizedIndex` to current max orderIndex
- If false: skip extraction
- "Request Summary" action always triggers extraction regardless of batch position

**Incremental Extraction**:
- Filter messages to only those with `orderIndex > lastSummarizedIndex`
- Pass existing summary as context (already supported by `extractSummary()` update mode)
- Reduces input tokens proportionally to conversation length

**Conversation Windowing**:
- In chat route, after building `conversationHistory`, slice to last 15 messages
- Inject current `currentSummaryJson` into the system prompt via `buildSystemPrompt()`
- New optional param: `currentSummary?: ProcessSummary` on `buildSystemPrompt()`
- When total messages < 15, send all (no windowing)

**Config Caching**:
- `src/lib/interview/config-cache.ts`: `Map<string, { data, systemPrompt, expiresAt }>`
- `getOrBuildConfig(projectId)`: returns cached config + compiled system prompt
- `invalidateConfig(projectId)`: clears cache entry
- TTL: 5 minutes (fallback safety net)
- Call `invalidateConfig()` from the configuration save endpoint

**MessageBubble Markdown Rendering**:
- Import `ReactMarkdown` and `remarkGfm` (already in dependencies)
- For assistant messages: wrap content in `<ReactMarkdown remarkPlugins={[remarkGfm]}>` with `prose prose-sm` classes
- For user messages: keep plain text with `whitespace-pre-wrap`
- Add `Bot` icon (lucide) for AI, `User` icon for employee
- Works progressively during streaming — React re-renders as content string grows

**System Prompt Formatting Instructions**:
- Add to `buildSystemPrompt()` after BEHAVIOR section:
  ```
  FORMATTING:
  - Use **bold** for key terms and process names
  - Use numbered lists when asking multiple follow-up questions
  - Use bullet points when recapping or summarizing captured information
  ```
